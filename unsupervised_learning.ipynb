{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cvicentm\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\past\\types\\oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable\n",
      "C:\\Users\\cvicentm\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\past\\builtins\\misc.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping\n",
      "C:\\Users\\cvicentm\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: \"\"\n",
      "C:\\Users\\cvicentm\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\lm\\counter.py:15: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence, defaultdict\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cvicentm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cvicentm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo ya había sido entrenado previamente...\n",
      "Se ha conseguido importar las variables: generator_normalize, Bow_matrix y vectorizer\n",
      "estamos con el tema de imprimir las gráficas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cvicentm\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estamos con el tema de terminar las gráficas\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cvicentm\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:69: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Apr 17 07:46:39 2020\n",
    "\n",
    "@author: cvicentm\n",
    "\"\"\"\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis.sklearn\n",
    "import pickle\n",
    "from create_corpus import create_corpus\n",
    "\n",
    "file_lda_model = 'pickle\\lda_model.sav'\n",
    "\n",
    "   \n",
    "n_topics = 25\n",
    "\n",
    "try:\n",
    "    \n",
    "    f=open(file_lda_model, 'rb')\n",
    "    lda = pickle.load(f)\n",
    "    print(\"El modelo ya había sido entrenado previamente...\")\n",
    "    generator_normalize = pickle.load(open(\"pickle\\generator_normalize.txt\", \"rb\"))\n",
    "    Bow_matrix = pickle.load(open(\"pickle\\Bow_matrix.txt\", \"rb\"))\n",
    "    vectorizer = pickle.load(open(\"pickle\\Vectorizer.txt\", \"rb\"))\n",
    "    print(\"Se ha conseguido importar las variables: generator_normalize, Bow_matrix y vectorizer\")\n",
    "    \n",
    "except IOError:\n",
    "    \n",
    "    try:\n",
    "        print(\"HA ENTRADO EN EL SEGUNDO TRY\")\n",
    "        f=open(file_lda_model, 'rb')\n",
    "        lda = pickle.load(f)\n",
    "        print(\"Se debe volver a crear el corpus\")\n",
    "        [generator_normalize, Bow_matrix, vectorizer]=create_corpus()\n",
    "        \n",
    "    except IOError:\n",
    "        print(\"AL FINAL HAY QUE HACERLO TODO\")\n",
    "        print(\"El modelo se debe entrenar ...\")\n",
    "        print(\"Creando el corpus\")\n",
    "        [generator_normalize, Bow_matrix, vectorizer]=create_corpus()\n",
    "        print(\"Proceso de creación del corpus finalizado\")\n",
    "        \n",
    "        lda = LatentDirichletAllocation(n_components=n_topics,max_iter=500,learning_method='batch',batch_size=50)\n",
    "        print(\"Se comienza a entrenar el corpus\")\n",
    "        lda.fit(Bow_matrix)\n",
    "        print(\"Finalización del entrenamiento del corpus completada\")\n",
    "        topics_per_document=lda.components_\n",
    "        \n",
    "        print(\"se va a proceder a guardar el modelo en un fichero\")\n",
    "        # if the number of subtitles doest change, we can use the same model than the last time\n",
    "        pickle.dump(lda, open(file_lda_model, 'wb'))\n",
    "     \n",
    "\n",
    "\n",
    "print(\"estamos con el tema de imprimir las gráficas\")\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(lda, Bow_matrix, vectorizer, mds='tsne')\n",
    "pyLDAvis.display(panel)\n",
    "print(\"estamos con el tema de terminar las gráficas\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for i in range(50):\n",
    "    topic_distribution = lda.transform(Bow_matrix[i])\n",
    "    numpy_distribution=np.asarray(topic_distribution)\n",
    "    numpy_distribution=np.resize(numpy_distribution,(n_topics,))\n",
    "    print(np.size(numpy_distribution))\n",
    "    fig, ax = plt.subplots()   # Declares a figure handle\n",
    "    ax.plot(np.arange(0,n_topics,1),numpy_distribution,'-*',label='mean topic proportion')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
